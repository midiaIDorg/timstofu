import functools
import math
import numba
import numpy as np
import numpy.typing as npt
import pandas as pd

from numba_progress import ProgressBar

from timstofu.numba_helper import get_min_int_data_type
from timstofu.numba_helper import inputs_series_to_numpy
from timstofu.numba_helper import np_uints
from timstofu.numba_helper import numba_wrap
from timstofu.stats import _count_unique
from timstofu.stats import count2D
from timstofu.stats import count_unique_for_indexed_data
from timstofu.stats import cumsum
from timstofu.stats import get_precumsums


@numba.njit(cache=True)
def _is_nondecreasing(xx):
    x_prev = -math.inf
    for x in xx:
        if x_prev > x:
            return False
        x_prev = x
    return True


is_nondecreasing = inputs_series_to_numpy(_is_nondecreasing)


@numba.njit
def _is_lex_nondecreasing(*arrays: npt.NDArray) -> bool:
    N = len(arrays[0])
    for arr in arrays:
        assert len(arr) == N
    for i in range(1, N):
        for arr in arrays:
            prev = arr[i - 1]
            curr = arr[i]
            if prev < curr:
                break  # go to next array
            elif prev > curr:
                return False  # out of order
            # else: equal, keep comparing next key
    return True


is_lex_nondecreasing = inputs_series_to_numpy(_is_lex_nondecreasing)


def test_is_lex_nondecreasing():
    assert _is_lex_nondecreasing([1, 1, 1, 2], [1, 2, 3, 1])
    assert _is_lex_nondecreasing([1, 1, 1, 2], [1, 2, 2, 1])
    assert not _is_lex_nondecreasing([1, 1, 1, 2], [1, 2, 1, 1])


@numba.njit
def _is_lex_strictly_increasing(*arrays: npt.NDArray) -> bool:
    N = len(arrays[0])
    for arr in arrays:
        assert len(arr) == N
    for i in range(1, N):
        strictly_greater_found = False
        for arr in arrays:
            prev = arr[i - 1]
            curr = arr[i]
            if prev < curr:
                strictly_greater_found = True
                break
            elif prev > curr:
                return False  # strictly decreasing: invalid
        if not strictly_greater_found:
            return False  # tuples are equal: not strictly increasing
    return True


def test_is_lex_strictly_increasing():
    assert _is_lex_strictly_increasing([1, 1, 1, 2], [1, 2, 3, 1])
    assert not _is_lex_strictly_increasing([1, 1, 1, 2], [1, 2, 2, 1])


is_lex_strictly_increasing = inputs_series_to_numpy(_is_lex_strictly_increasing)


@numba.njit(boundscheck=True)
def deduplicate(
    sorted_tofs: npt.NDArray,
    sorted_intensities: npt.NDArray,
    nondeduplicated_counts: npt.NDArray,
    deduplicated_event_count: int,
    progress_proxy: ProgressBar | None = None,
    dedup_tofs: npt.NDArray | None = None,
    dedup_intensities: npt.NDArray | None = None,
):
    """
    Deduplicates time-of-flight (TOF) and intensity data by aggregating intensities
    of duplicate TOF values within predefined groups.

    This function assumes the input arrays are sorted by TOF and that events are
    grouped such that each group has a known number of entries (provided by
    `nondeduplicated_counts`). Within each group, TOFs are expected to be
    nondecreasing. For repeated TOFs, intensities are summed.

    This doc string is autogenerated and is a simple shallow code analysis that we wanted and ChatGPT gave us.

    Parameters
    ----------
    sorted_tofs : np.ndarray
        1D array of sorted time-of-flight values.
    sorted_intensities : np.ndarray
        1D array of intensities corresponding to each TOF in `sorted_tofs`.
    nondeduplicated_counts : np.ndarray
        1D array indicating the number of events in each group (before deduplication).
    deduplicated_event_count : int
        Total number of unique events expected after deduplication.
    progress_proxy : ProgressBar or None, optional
        An optional progress bar object with an `.update(n: int)` method.
    dedup_tofs : np.ndarray or None, optional
        Optional output array to store deduplicated TOFs. If None, a new array is allocated.
    dedup_intensities : np.ndarray or None, optional
        Optional output array to store deduplicated intensities. If None, a new array is allocated.

    Returns
    -------
    dedup_tofs : np.ndarray
        Array of deduplicated TOF values.
    dedup_intensities : np.ndarray
        Array of intensities corresponding to the deduplicated TOFs.

    Raises
    ------
    AssertionError
        If the number of deduplicated events or group counts does not match expectations.
    """
    if dedup_tofs is None:
        dedup_tofs = np.empty(
            dtype=sorted_tofs.dtype,
            shape=deduplicated_event_count,
        )

    if dedup_intensities is None:
        dedup_intensities = np.zeros(
            dtype=sorted_intensities.dtype,
            shape=deduplicated_event_count,
        )

    counts_idx = 0
    current_group_count = 0
    dedup_idx = -1
    prev_tof = -math.inf

    for i, (tof, intensity) in enumerate(zip(sorted_tofs, sorted_intensities)):
        if current_group_count == nondeduplicated_counts[counts_idx]:
            counts_idx += 1
            current_group_count = 0
            prev_tof = -math.inf  # force top > prev_tof

        if tof > prev_tof:
            dedup_idx += 1
            dedup_tofs[dedup_idx] = tof

        dedup_intensities[dedup_idx] += intensity
        prev_tof = tof
        current_group_count += 1
        if progress_proxy is not None:
            progress_proxy.update(1)

    assert dedup_idx == deduplicated_event_count - 1
    assert counts_idx == len(nondeduplicated_counts) - 1

    return dedup_tofs, dedup_intensities


# this should be wrapped.


@numba.njit(boundscheck=True)
def _lexargcountsort2D(
    xx: npt.NDArray,
    yy: npt.NDArray,
    x_y_to_cumsum: npt.NDArray,
    copy: bool = True,
):
    """Get the order sorting xx and yy lexicographically.

    Arguments:
        xx  (np.array): Array of any subptype of np.integer.
        yy  (np.array): Array of any subptype of np.integer.
        x_y_to_cumsum (np.array): 2D array mapping x,y to cumulated sum of occurrences of x,y in xx and yy.
    """
    assert len(xx.shape) == 1
    assert xx.shape == yy.shape
    assert len(x_y_to_cumsum.shape) == 2
    if copy:
        x_y_to_cumsum = x_y_to_cumsum.copy()
    output = np.empty(
        dtype=x_y_to_cumsum.dtype,
        shape=xx.shape,
    )
    # going backwards preserves input order in (frame,scan) groups
    for i in range(len(xx) - 1, -1, -1):
        x = xx[i]
        y = yy[i]
        # do not remove this cast: uint-int->float in numpy.
        x_y_to_cumsum[x, y] -= np.uint32(1)
        idx = x_y_to_cumsum[x, y]
        assert idx >= 0, "Circular boom!!!"
        assert idx < len(xx), "idx beyond scope"
        output[idx] = i
    return output


@functools.wraps(_lexargcountsort2D)
@inputs_series_to_numpy
def lexargcountsort2D(xx: npt.NDArray, yy: npt.NDArray, *args):
    assert np.issubdtype(xx.dtype, np.integer)
    assert np.issubdtype(yy.dtype, np.integer)
    return _lexargcountsort2D(xx, yy, *args)


def test_lexargcountsort2D():
    frames = np.array([1, 1, 2, 2, 3])
    scans = np.array([1, 1, 3, 4, 2])

    rev_frames = frames[::-1]
    rev_scans = scans[::-1]

    frame_scan_to_count, *frame_scan_ranges = count2D(rev_frames, rev_scans)
    our_order = lexargcountsort2D(
        rev_frames,
        rev_scans,
        cumsum(frame_scan_to_count),
    )
    expected_order = np.lexsort((rev_scans, rev_frames))
    # np.testing.assert_array_equal(our_order, expected_order)
    # this here won't work: np.lexsort is not stable.
    # need to evaluate on test data.
    for xx in (frames, scans):
        np.testing.assert_array_equal(xx[our_order], xx[expected_order])


@numba.njit(boundscheck=True)
def lexargcountsort2D_to_3D(
    xy_to_first_idx: npt.NDArray,
    xy_to_count: npt.NDArray,
    xy_presorted_zz: npt.NDArray,
    xy_order: npt.NDArray,
    copy: bool = True,
) -> npt.NDArray:
    """Complete the sort."""
    xy_to_xyz_order = (
        np.empty(
            dtype=xy_to_first_idx.dtype,
            shape=xy_presorted_zz.shape,
        )
        if copy
        else xy_order
    )
    for x, y in zip(*xy_to_count.nonzero()):
        s = xy_to_first_idx[x, y]
        e = s + xy_to_count[x, y]
        order_z_between_s_and_e = np.argsort(xy_presorted_zz[s:e])
        # this is not stable!!! so what?
        xy_to_xyz_order[s:e] = xy_order[s:e][order_z_between_s_and_e]
    return xy_to_xyz_order


# TODO: make it possible to use user allocated arrays for results
@inputs_series_to_numpy
def argcountsort3D(
    xx: npt.NDArray | pd.Series,
    yy: npt.NDArray | pd.Series,
    zz: npt.NDArray | pd.Series,
    return_counts: bool = False,
) -> npt.NDArray | tuple[npt.NDArray, npt.NDArray, npt.NDArray]:
    """Get the order sorting any array as long as the provided integer arrays lexicographically by xx, yy, and zz.

    Parameters
    ----------
    xx (np.array): A 1D array.
    yy (np.array): A 1D array.
    zz (np.array): A 1D array.
    return_counts (bool): Return also xy2count.

    Returns
    ```````
    np.array|tuple[np.array,np.array,np.array]: the reindexing needed to establish the lexicogrpahical order or that and counts and index arrays.

    Notes
    -----
    If order is the results, then tuples zip(xx[order], yy[order], zz[order]) will be lexicographically sorted (nondecreasing).
    """
    xy2count, *_ = count2D(xx, yy)
    xy2first_idx = get_precumsums(xy2count)
    xy_order = lexargcountsort2D(
        xx,
        yy,
        xy2first_idx + xy2count,
        False,
    )
    xyz_order = lexargcountsort2D_to_3D(
        xy2first_idx,
        xy2count,
        zz[xy_order],
        xy_order,
        False,
    )
    if return_counts:
        return xyz_order, xy2count, xy2first_idx
    return xyz_order


def test_argcountsort3D():
    xx = np.array([2, 2, 2, 1, 1, 2, 2, 3, 3])
    yy = np.array([2, 2, 2, 5, 4, 3, 4, 2, 1])
    zz = np.array([3, 2, 12, 355, 424, 23, 4, 2, 1])

    order = argcountsort3D(xx, yy, zz)
    expected_order = np.lexsort((zz, yy, xx))

    for tt in (xx, yy, zz):
        np.testing.assert_array_equal(
            tt[order],
            tt[expected_order],
        )


def test_count_unique_for_indexed_data():
    xx = np.array([1, 1, 1, 1, 2, 2, 2])
    yy = np.array([2, 1, 2, 1, 1, 2, 1])
    zz = np.array([2, 1, 2, 2, 1, 2, 1])

    order, xy2count, xy2first_idx = argcountsort3D(xx, yy, zz, return_counts=True)
    assert is_lex_nondecreasing(xx[order], yy[order], zz[order])

    mega_cast = lambda type: lambda xx: tuple(map(type, xx))
    res = set(
        map(
            mega_cast(int),
            zip(
                *count_unique_for_indexed_data(
                    zz[order], xy2count, xy2first_idx
                ).nonzero()
            ),
        )
    )
    expected_res = set(map(mega_cast(int), zip(xx, yy)))
    assert res == expected_res


@numba.njit
def rank_array(order):
    ranks = np.empty(len(order), dtype=np.int32)
    for rank, i in enumerate(order):
        ranks[i] = rank
    return ranks


def _grouped_argsort(xx, group_index, order=None):
    """Sort xx in groups

    Parameters
    ----------
    xx (np.array): to be argsorted, grouped by index.
    grouped_index (np.array): 1D array with counts, one field larger than the number of different values of the grouper.

    Notes
    -----
    `group_index[i]:group_index[i+1]` returns a view into all members of the i-th group.
    """
    if order is None:
        order = np.empty(len(xx), dtype=np.uint32)
    for i in numba.prange(len(group_index) - 1):
        s = group_index[i]
        e = group_index[i + 1]
        order[s:e] = s + np.argsort(xx[s:e])
    return order


grouped_argsorts = numba_wrap(_grouped_argsort)
grouped_argsort = grouped_argsorts["safe", "multi_threaded"]


@numba.njit
def merge_uints(arrays, maxes, res):
    """
    Encode multiple uints into a single unique uint per row using Horner's method.

    This function encodes N-dimensional integer coordinates into a single integer index,
    similar to row-major flattening of a multi-dimensional array. It assumes that for each
    dimension `j`, values in `arrays[j]` are in the range `[0, maxes[j])`.

    Parameters
    ----------
    arrays : list of 1D arrays of int
        A list of arrays, each of length `M`, representing values along each dimension.
        Each `arrays[j][i]` corresponds to the j-th coordinate of the i-th element.
    maxes : 1D array of int
        The maximum value (exclusive upper bound) for each dimension. Used as the radix
        in Hornerâ€™s method for flattening.
    res : 1D array of int
        Output array of length `M`, where the resulting single indices will be stored.

    Returns
    -------
    res : 1D array of int
        The same array passed in as `res`, containing the combined indices.

    Notes
    -----
    This is equivalent to computing:

        res[i] = arrays[0][i] * (maxes[1] * maxes[2] * ...) +
                 arrays[1][i] * (maxes[2] * ...) + ... +
                 arrays[N-1][i]

    which uniquely encodes each N-dimensional index into a flat integer.
    """
    for i in range(len(arrays[0])):
        res[i] = 0
        for j in range(len(arrays)):
            res[i] *= maxes[j]
            res[i] += arrays[j][i]
    return res


@numba.njit(boundscheck=True)
def lexargcountsort(
    xx: npt.NDArray,
    xx_cumsum: npt.NDArray,
    order: npt.NDArray | None = None,
    copy_cumsum: bool = True,
):
    """Get the order sorting xx by values in group_index."""
    assert len(xx.shape) == 1
    assert xx_cumsum[-1] == len(xx)
    if order is None:
        order = np.empty(
            dtype=np.uint32,
            shape=xx.shape,
        )
    xx_cumsum = xx_cumsum.copy() if copy_cumsum else xx_cumsum
    # going backwards preserves input order in (frame,scan) groups
    for i in range(len(xx) - 1, -1, -1):
        x = xx[i]
        xx_cumsum[x] -= np.uint32(1)  # !!! uint-int->float in numpy !!!
        j = xx_cumsum[x]
        assert j >= 0, "Circular boom!!!"
        assert j < len(xx), "idx beyond scope"
        order[j] = i
    return order


@numba.njit
def zip_uints_into_bigger_uint(
    uint_arrays,
    array_maxes,
    start_idx,
    end_idx,
    results,
):
    """Zip uints into a bigger int using Horner scheme."""
    assert start_idx <= end_idx
    assert end_idx <= len(results)
    for i in range(start_idx, end_idx):
        results[i] = 0
        for j in range(len(uint_arrays)):
            results[i] *= array_maxes[j]
            results[i] += uint_arrays[j][i]


@numba.njit(boundscheck=True, parallel=True)
def _countlexargsort(arrays, array_maxes, group_index, order) -> npt.NDArray:
    """Sort arrays.

    Parameters
    ----------
    arrays (tuple): A tuple of arrays to be argsorted, grouped by index.
    grouped_index (np.array): 1D array with counts, one field larger than the number of different values of the grouper.

    Notes
    -----
    `group_index[i]:group_index[i+1]` returns a view into all members of the i-th group.
    """
    assert len(arrays) > 0
    for i in numba.prange(len(group_index) - 1):
        s = group_index[i]
        e = group_index[i + 1]
        zip_uints_into_bigger_uint(arrays, array_maxes, s, e, order)
        order[s:e] = s + np.argsort(order[s:e])
    return order


def grouped_lexargcountsort(
    arrays: tuple[npt.NDArray, ...],
    group_index: npt.NDArray,
    array_maxes: tuple[int, ...] | None = None,
    order: npt.NDArray | None = None,
):
    assert len(group_index.shape) == 1
    size = group_index[-1]
    for arr in arrays:
        assert len(arr.shape) == 1, "Arrays must be 1D."
        assert len(arr) == size, "Arrays must be equaly sized."
        assert arr.dtype in np_uints, "Arrays must be some version of np.uint"
    if order is None:
        order = np.empty(
            shape=size,
            dtype=get_min_int_data_type(math.prod(array_maxes)),
        )
    if array_maxes is None:
        array_maxes = (arr.max() for arr in arrays)
    return _countlexargsort(arrays, array_maxes, group_index, order)
